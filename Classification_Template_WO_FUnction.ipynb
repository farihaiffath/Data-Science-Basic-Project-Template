{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134b810f-699c-45d8-91e7-ae3d5912763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 1. Configuration and Setup\n",
    "# ============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95bc30f-7259-45b4-b405-bd5b2ff867a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Define paths\n",
    "DATA_PATH = \"data/\"  # Path to local data files\n",
    "OUTPUT_PATH = \"output/\"  # Path to save results\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)  # Create output folder if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d117fe41-8768-40a6-b170-29f9b8221223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 2. Data Loading\n",
    "# ============================\n",
    "\n",
    "# Option 1: Load data from a local file\n",
    "file_path = os.path.join(DATA_PATH, \"sample.csv\")  # Replace with your file name\n",
    "data = pd.read_csv(file_path)  # Adjust to read_excel, read_json, etc., if necessary\n",
    "print(\"Data loaded from local file.\")\n",
    "\n",
    "# Uncomment the following block if loading from a web URL is required instead\n",
    "# url = \"https://example.com/sample.csv\"  # Replace with your data URL\n",
    "# response = requests.get(url)\n",
    "# data = pd.read_csv(StringIO(response.text))\n",
    "# print(\"Data loaded from URL.\")\n",
    "\n",
    "# Uncomment the following block if loading via web scraping is required\n",
    "# web_url = \"https://example.com/sample_table\"  # Replace with your web scraping target URL\n",
    "# response = requests.get(web_url)\n",
    "# soup = BeautifulSoup(response.text, 'html.parser')\n",
    "# table = soup.find('table')  # Find the table in the webpage\n",
    "# data = pd.read_html(str(table))[0]  # Convert the table into a DataFrame\n",
    "# print(\"Data loaded from web scraping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c55e2d-fcb0-4318-b422-87a5586c0f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 3. Data Preprocessing\n",
    "# ============================\n",
    "\n",
    "print(f\"Initial Data Shape: {data.shape}\")\n",
    "\n",
    "# 3.1 Handle Missing Values\n",
    "# Drop columns with more than 80% missing data\n",
    "data = data.dropna(thresh=int(0.8 * len(data)), axis=1)\n",
    "\n",
    "# Fill remaining missing values with column means (numerical) or mode (categorical)\n",
    "for col in data.columns:\n",
    "    if data[col].dtype in ['float64', 'int64']:\n",
    "        data[col].fillna(data[col].mean(), inplace=True)\n",
    "    elif data[col].dtype == 'object':\n",
    "        data[col].fillna(data[col].mode()[0], inplace=True)\n",
    "\n",
    "print(f\"Shape after handling missing values: {data.shape}\")\n",
    "\n",
    "# 3.2 Handle Outliers (Remove data points beyond 1.5*IQR range for numerical features)\n",
    "for col in data.select_dtypes(include=['float64', 'int64']).columns:\n",
    "    Q1 = data[col].quantile(0.25)\n",
    "    Q3 = data[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    data = data[(data[col] >= lower_bound) & (data[col] <= upper_bound)]\n",
    "\n",
    "print(f\"Shape after outlier removal: {data.shape}\")\n",
    "\n",
    "# 3.3 Feature Engineering\n",
    "# Create new features (example: total, ratio, etc.)\n",
    "if 'feature1' in data.columns and 'feature2' in data.columns:\n",
    "    data['feature_ratio'] = data['feature1'] / (data['feature2'] + 1e-6)\n",
    "    data['feature_sum'] = data['feature1'] + data['feature2']\n",
    "    print(\"New features created.\")\n",
    "\n",
    "# 3.4 Encode Categorical Variables\n",
    "# One-hot encode categorical variables\n",
    "data = pd.get_dummies(data, drop_first=True)\n",
    "print(\"Categorical variables encoded.\")\n",
    "\n",
    "# 3.5 Feature Scaling\n",
    "# Normalize numerical features using Min-Max Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "numeric_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "data[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n",
    "print(\"Numerical features scaled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e88377-8273-46da-b90f-8aab6b9605cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 4. Exploratory Data Analysis (EDA)\n",
    "# ============================\n",
    "\n",
    "# 4.1 Statistical Summaries\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(data.describe())\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# 4.2 Correlation Analysis\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = data.corr()\n",
    "\n",
    "# Display the correlation heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# 4.3 Feature Distributions\n",
    "# Plot distributions for all numerical columns\n",
    "for column in data.select_dtypes(include='number').columns:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.histplot(data[column], kde=True)\n",
    "    plt.title(f\"Distribution of {column}\")\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "# 4.4 Class Balance (for classification tasks)\n",
    "if 'target' in data.columns:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.countplot(x='target', data=data)\n",
    "    plt.title(\"Class Distribution\")\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "# 4.5 Pairwise Relationships (for small datasets)\n",
    "# Visualize pairwise relationships between features\n",
    "if data.shape[1] <= 10:  # Limit to small datasets\n",
    "    sns.pairplot(data, diag_kind=\"kde\")\n",
    "    plt.show()\n",
    "\n",
    "# 4.6 Detect Multicollinearity\n",
    "# Identify highly correlated features (above a threshold, e.g., 0.9)\n",
    "threshold = 0.9\n",
    "high_corr_features = set()\n",
    "for i in range(correlation_matrix.shape[0]):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
    "            col_name = correlation_matrix.columns[i]\n",
    "            high_corr_features.add(col_name)\n",
    "\n",
    "print(f\"Highly correlated features (above {threshold}): {high_corr_features}\")\n",
    "\n",
    "# 4.7 Visualizing Outliers (Boxplots)\n",
    "for column in data.select_dtypes(include='number').columns:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.boxplot(x=data[column])\n",
    "    plt.title(f\"Boxplot of {column}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746eb06d-6f9a-4ff8-bc50-5cabfeaf4ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 5. Train-Test Split\n",
    "# ============================\n",
    "\n",
    "# Define features and target\n",
    "target_column = \"target\"  # Replace with your target column\n",
    "features = data.drop(columns=[target_column])\n",
    "target = data[target_column]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4824992d-8896-4a4f-a0e0-414c4e8cae30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 6. Model Training\n",
    "# ============================\n",
    "\n",
    "# Initialize and train a Random Forest model\n",
    "model = RandomForestClassifier(random_state=SEED)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7268972-f0e9-4222-b1e8-5a068c3eedd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 7. Model Evaluation\n",
    "# ============================\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, predictions):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
